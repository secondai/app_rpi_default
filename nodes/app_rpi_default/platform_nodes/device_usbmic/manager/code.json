{
  "type": "code:0.0.1:local:32498h32f2",
  "name": "code",
  "data": {
    "key": "078a6e76-f623-4ae7-8681-f672b2a23c8a",
    "code": "(()=>{\n  return new Promise(async (resolve, reject)=>{\n    try {\n      \n      // Expecting INPUT = device_input_node:Qmf8392h3298h\n      // - capabilityNode: Object,\n      // - externalInputNode: Object,\n\n      // let dataNode = {\n      //   type: 'device_input_node:Qmf8392h3298h',\n      //   data: {\n      //     deviceNode: deviceNode,\n      //     inputNode: {\n      //       type: 'standard_device_action:Qm2398fhsnsf',\n      //       data: {\n      //         action: 'startup',\n      //         options: {}\n      //       }\n      //     }\n      //   }\n      // }\n      \n      // Load the \"Devices\" capability\n      \n      let cmdInputNode = INPUT.data.inputNode;\n\n      if(cmdInputNode.type != 'standard_device_action:Qm2398fhsnsf'){\n        console.log('Unexpected input schema for device input');\n        return reject('Unexpected input schema for device input');\n      }\n      \n      let inputAction = cmdInputNode.data.action;\n      let inputOpts = cmdInputNode.data.options;\n      \n      \n      \n      // Acts similar to \"incoming_from_uni\" \n      // - handles all(?) requests to the device \n      //   - usually passes off to another function \n      \n      // Get settings for device \n      // - expecting consistent relative location \n      let thisNode = await universe.searchMemory({\n        filter: {\n          dataFilter: {\n            _id: SELF._id,\n          }\n        }\n      });\n      thisNode = thisNode[0];\n      let deviceNode = thisNode.parent.parent;\n      let settingsNode = universe.lodash.find(deviceNode.nodes,{type: 'settings:Qmf329ffhj9f823h'});\n      if(!settingsNode){\n        console.error('Missing settings for device', deviceNode.data.name);\n        return reject({});\n      }\n      \n      // const Sound = universe.require('node-aplay');\n      \n      // var say = universe.require('say');\n      \n      // Actions \n    \n      switch(inputAction){\n        case 'startup':\n          // check for existence of mic (hardware device profile matches, address exists, etc) \n          console.log('usb mic startup');\n          return resolve({\n            type: 'boolean:Qmdsfdlkj',\n            data: true\n          });\n          break;\n          \n        case 'input':\n          // got some type of input\n          console.log('inputOpts:', JSON.stringify(inputOpts.action,null,2));\n          switch(inputOpts.action){\n            case 'listen':\n              // starts listening and transcribing \n              // - returns/pipes result, doesn't \"hand off\" to a command, etc. \n              \n              console.log('Starting listen attempt1');\n              \n              const record = universe.require('node-record-lpcm16');\n              \n              var speech = universe.require('@google-cloud/speech'); \n              const spawn = universe.require('child_process').spawn;\n              \n              // process.env.GOOGLE_APPLICATION_CREDENTIALS = __dirname + '/gcreds.json';\n              let credentials = {\n                \"type\": \"service_account\",\n                \"project_id\": \"second-1208\",\n                \"private_key_id\": \"7b014d91d329bbf45b88d6b3e10103ba04b6450b\",\n                \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQC99jLWgGw61OAA\\nWZH/RyBzX6AWOKvskaGqBjs1laVteMEqDelIozNp0we12zgiBcyoq6VkfgvH32sw\\n7H58ZmDhtvkjQ1RRL+wc9ryQFhjO1TyY1dbgBqnyzRmWt8Pw+cnzl7hY17DFFpT9\\ncK9mxM5pqvg7ZOSxeJ7nu6JPhtTwMgklRg4/U0ph8Z+iZxolrNDwEN9TgZlxOf/3\\nXY88KfSfTsC6HYhHtUtQeaoIV+gRL+hf15gOn+E767zn3n3wwzb8doU2hoRqTgSf\\nerZWRcrJhfKfs3/1irDKdTplQ2gf2n7l55iCy1qy24J3gH554RxY+os2gnsT+pho\\nEguCXz0tAgMBAAECgf93Ok+b9sf3biUFgOlhtN5EsFuuu2MtIYPfl/da2D/OtfLQ\\njdhm/uNPSGS8NolXGoOZx9jrwzEhuP1y5UXHczFt586Jk3jPV6lRhIvDLPbAkbqS\\nV1vrCWhGPKvJv5+yzaczoX1lakSSAZFLE2XfSJT8YiUP/bpDO9aA0t9fQbguCQA9\\nMrdGClSPSG00MnZB4Pto87rEalbB7bMkiZ8MfuBkVtIEUrUl7j40gYku8hKYOVTU\\n4GykYeNJdgMEHsZITSsfF3jQir0y6wotEKB0WAMAzus+Kza5mhXm6K2EmTgLm5+b\\nQkcDmgP2trh2OVzgS50zvkGVwvtHARrbsnKpTZkCgYEA3RaEpQkCMrH0cK+iIsGj\\nKE4rJY28Uv5iu9pwHUZNT2OgWEpcUgHK3XPRFlUJvrCXn63UMCNKrxRSAoE7B901\\n8NFNVBt5OKJn1B8IjyNF6PaJpIqTFjvwDySuw1BwD3Ed7NahpIfg/ihRlZt/p0o3\\nLMgSaBm3MLJ1qIqW2/nXVBkCgYEA2/Vm1q3MzTbzIjkv55Z6gubaaFajei5cUB6Z\\nCbFk0OKSPqwu5uJm/UPhJsYOMdk1cznndTRNztG/1Nb6vYdz+Wrf3h8vApArdOsG\\nwLBJZVb/19+c+4/nhdEHfYOSKHxBnzQphD7vUl2QD76sZMSqH0DnLem5mhhArAet\\nHN4b9DUCgYEAtDRohcRkAEJAhaECvOWsQWwFiySN/e/PNmMDwzjMRwtjZNOnkEhZ\\nvEtsf5Fs0ftKeyLKsznw4+fiOJxxKyXJk7JmH0oNcnYdvy5UYadUkmBxLKau4Q6V\\nyyIROK3VySBgTe1b76m5YDAo35LRZ9/8PQoOxr8/K/gqIuhjsoVgV/ECgYB5amzp\\n8+YpITv0mDypSZc2ytek4K48s3qhf0Ja52EPLQItNj5tlGvlQaL3FDyhp5Spljz9\\npXGNyJprkn+2Px71ftk0c2IvuAVE9Lc1hhn9pphzQSwWcMkMROYtkoMLdguDJfgE\\n1/ijLqJnDol61lRSDDGQVoPtW5XXYCVe2fv0QQKBgQC8nHOdaq8uhK4l4gd22VaQ\\nIb1rL/8GQeqk6OF3aOmDHq4GdfxjJN65wUxfHvLCBHc6Q6kvQzA9JhlsDcgokCvE\\nCqOiPZvbisIVpCC1zFqSffVyi9VVJITn1n2wSMz1TN1mI5Bu3eDccLD1qIuScm6h\\nwGsu7D2j2u5sCBuLJuxfFw==\\n-----END PRIVATE KEY-----\\n\",\n                \"client_email\": \"256309733057-compute@developer.gserviceaccount.com\",\n                \"client_id\": \"100809297210576165766\",\n                \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n                \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n                \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n                \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/256309733057-compute%40developer.gserviceaccount.com\"\n              }\n\n              \n              \n              // Creates a client\n              const client = new speech.SpeechClient({\n                // apiKey: 'AIzaSyB6KYZNp5LReVN3aSlYGoVXqcfHHdYKmLs',\n                projectId: 'second-1208',\n                credentials\n              });\n                          \n              const encoding = 'LINEAR16';\n              const sampleRateHertz = 16000;\n              const languageCode = 'en-US';\n            \n              const request = {\n                config: {\n                  encoding: encoding,\n                  sampleRateHertz: sampleRateHertz,\n                  languageCode: languageCode,\n                },\n                interimResults: true, // If you want interim results, set this to true\n              };\n            \n              // Create a recognize stream\n              const recognizeStream = client\n                .streamingRecognize(request)\n                .on('error', console.error)\n                .on('data', data =>{\n                  // console.log(\n                  //   data.results[0] && data.results[0].alternatives[0]\n                  //     ? `Transcription: ${data.results[0].alternatives[0].transcript}\\n`\n                  //     : `\\n\\nReached transcription time limit\\n`\n                  // )\n                  if(data.results && data.results.length){\n                    let result = data.results[0];\n                    // console.log('Results:', JSON.stringify(data.results,null,2));\n                    if(result.isFinal){\n                      console.log('Final:', result.alternatives[0].transcript, result.alternatives[0].confidence)\n                      // makes sure listening stops\n                      console.log('stopping recording...');\n                      universe.clearTimeout(recordTimeout);\n                      record.stop();\n                      console.log('stopped recording');\n                    } else {\n                      console.log('Temp:', result.alternatives[0].transcript, result.alternatives[0].confidence);\n                    }\n                  }\n                });\n            \n              // Start recording and send the microphone input to the Speech API\n              record\n                .start({\n                  sampleRateHertz: sampleRateHertz,\n                  threshold: 0,\n                  // Other options, see https://www.npmjs.com/package/node-record-lpcm16#options\n                  verbose: false,\n                  recordProgram: 'arecord', // Try also \"arecord\" or \"sox\" or \"rec\"\n                  silence: '10.0',\n                  device: 'plughw:CARD=Device,DEV=0'\n                })\n                .on('error', console.error)\n                .pipe(recognizeStream);\n            \n\n              var recordTimeout = universe.setTimeout(()=>{\n                console.log('stopping recording (timeout)...');\n                record.stop();\n                console.log('stopped');\n              },10 * 1000);\n              // var request = {\n              //   config: { \n              //     encoding: 'LINEAR16',\n              //     sampleRateHertz: 16000,\n              //     languageCode: 'en-US'\n              //   },\n              //   singleUtterance: false, \n              //   interimResults: true \n              // }; \n              \n              // var busy = false; \n              // function utterToMe() {\n              //   if (busy == true) { \n              //     return; \n              //   }\n               \n              //   busy = true; \n              //   const arecord = spawn('arecord', ['-f','S16_LE','-r','16000', '-D','plughw:CARD=Device,DEV=0'], {}, () => { console.log(\"Started listening...\"); });\n              \n              \n              //   // Stream the audio to the Google Cloud Speech API\n              //   const recognizeStream = client\n              //     .streamingRecognize(request)\n              //     .on('error', console.error)\n              //     .on('data', data => {\n              //       console.log('Got data');\n              //       if(data.results && \n              //         data.results.length && \n              //         data.results[0].alternatives &&\n              //         data.results[0].alternatives.length){\n              //         console.log(\n              //           `Transcription: ${data.results[0].alternatives[0].transcript}`\n              //         );\n              //       }\n              //       // console.log(\n              //       //   `Transcription: ${data.results[0].alternatives[0].transcript}`\n              //       // );\n              //     });\n              \n              //   // // Stream an audio file from disk to the Speech API, e.g. \"./resources/audio.raw\"\n              //   // fs.createReadStream(filename).pipe(recognizeStream);\n              \n              //   console.log(\"Kicked off transcribing process.\"); \n              //   arecord.stdout.pipe(recognizeStream)\n                \n              //     // .on('error', console.error) \n              //     // .on('data', function(data) { \n              //     //   if (data.results) { \n              //     //     console.log(data.results);\n              //     //   }\n              //     // });\n              \n              //   universe.setTimeout(() => { \n              //     console.log(\"Done with current recognition.\");\n              //     arecord.kill();\n              //     busy = false;\n              //   }, 5000);\n              // }\n              // utterToMe();\n              \n              return resolve({\n                type: 'boolean:Qmdsfljk',\n                data: true\n              });\n            \n            default:\n              console.log('Unhandled inputOpts.action type:', inputOpts.action);\n              break;\n              \n          }\n          return resolve({\n            type: 'boolean:Qmdsfdlkj',\n            data: true\n          });\n          break;\n          \n        default:\n          console.error('Invalid action type:', inputAction);\n          return reject({});\n      }\n        \n    }catch(err){\n      console.error('err managing device:', err);\n      resolve({ERROR: true, err: err.toString()});\n    }\n    \n    \n  })\n})()"
  }
}