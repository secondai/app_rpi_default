{
  "type": "code:0.0.1:local:32498h32f2",
  "name": "code",
  "data": {
    "key": "078a6e76-f623-4ae7-8681-f672b2a23c8a",
    "code": "(()=>{\n  return new Promise(async (resolve, reject)=>{\n    try {\n      \n      // Expecting INPUT = device_input_node:Qmf8392h3298h\n      // - capabilityNode: Object,\n      // - externalInputNode: Object,\n\n      // let dataNode = {\n      //   type: 'device_input_node:Qmf8392h3298h',\n      //   data: {\n      //     deviceNode: deviceNode,\n      //     inputNode: {\n      //       type: 'standard_device_action:Qm2398fhsnsf',\n      //       data: {\n      //         action: 'startup',\n      //         options: {}\n      //       }\n      //     }\n      //   }\n      // }\n      \n      // Load the \"Devices\" capability\n      \n      let cmdInputNode = INPUT.data.inputNode;\n\n      if(cmdInputNode.type != 'standard_device_action:Qm2398fhsnsf'){\n        console.log('Unexpected input schema for device input');\n        return reject('Unexpected input schema for device input');\n      }\n      \n      let inputAction = cmdInputNode.data.action;\n      let inputOpts = cmdInputNode.data.options;\n      \n      \n      \n      // Acts similar to \"incoming_from_uni\" \n      // - handles all(?) requests to the device \n      //   - usually passes off to another function \n      \n      // Get settings for device \n      // - expecting consistent relative location \n      let thisNode = await universe.searchMemory({\n        filter: {\n          dataFilter: {\n            _id: SELF._id,\n          }\n        }\n      });\n      thisNode = thisNode[0];\n      let deviceNode = thisNode.parent.parent;\n      let settingsNode = universe.lodash.find(deviceNode.nodes,{type: 'settings:Qmf329ffhj9f823h'});\n      if(!settingsNode){\n        console.error('Missing settings for device', deviceNode.data.name);\n        return reject({});\n      }\n      \n      let googleCredsNode = await universe.searchMemory({\n        filter: {\n          dataFilter: {\n            type: {\n              $like: 'credentials_google:'\n            }\n          }\n        }\n      });\n      googleCredsNode = googleCredsNode.length ? googleCredsNode[0]:null;\n      \n      \n      let snowboyCredsNode = await universe.searchMemory({\n        filter: {\n          dataFilter: {\n            type: {\n              $like: 'credentials_snowboy:'\n            }\n          }\n        }\n      });\n      snowboyCredsNode = snowboyCredsNode.length ? snowboyCredsNode[0]:null;\n      \n\n      const record = universe.require('node-record-lpcm16');\n      const Detector = universe.require('snowbow').Detector;\n      const Models = universe.require('snowbow').Models;\n      \n      var speech = universe.require('@google-cloud/speech'); \n      const spawn = universe.require('child_process').spawn;\n      \n\n      async function playSound(eventName){\n        console.log('playSound:', eventName);\n        await universe.loadAndRunCapability('Speaker',{},{\n          type: 'standard_capability_action:0.0.1:local:298j291bs',\n          data: {\n            action: 'play-sound-for-event',\n            options: {\n              type: 'generic_event:...',\n              data: {\n                event: eventName\n              }\n            }\n          }\n        });\n      }\n      \n      // const Sound = universe.require('node-aplay');\n      \n      // var say = universe.require('say');\n      \n      let doneCanWipe;\n              \n      // Actions \n    \n      switch(inputAction){\n        case 'startup':\n          // check for existence of mic (hardware device profile matches, address exists, etc) \n          console.log('usb mic startup');\n          return resolve({\n            type: 'boolean:Qmdsfdlkj',\n            data: true\n          });\n          break;\n          \n        case 'input':\n          // got some type of input\n          console.log('inputOpts:', JSON.stringify(inputOpts.action,null,2));\n          \n          switch(inputOpts.action){\n            case 'listen-for-hotword':\n              \n              const models = new Models();\n              \n              models.add({\n                file: 'resources/models/snowboy.umdl', // staticFilePath + '/jillian.pmdl'\n                sensitivity: '0.5',\n                hotwords : 'snowboy'\n              });\n              \n              const detector = new Detector({\n                resource: \"resources/common.res\",\n                models: models,\n                audioGain: 2.0,\n                applyFrontend: true\n              });\n              \n              detector.on('silence', function () {\n                console.log('silence');\n              });\n              \n              detector.on('sound', function (buffer) {\n                // <buffer> contains the last chunk of the audio that triggers the \"sound\"\n                // event. It could be written to a wav stream.\n                console.log('sound');\n              });\n              \n              detector.on('error', function () {\n                console.log('error');\n              });\n              \n              detector.on('hotword', function (index, hotword, buffer) {\n                // <buffer> contains the last chunk of the audio that triggers the \"hotword\"\n                // event. It could be written to a wav stream. You will have to use it\n                // together with the <buffer> in the \"sound\" event if you want to get audio\n                // data after the hotword.\n                console.log('--hotword trigger--');\n                console.log(buffer);\n                console.log('hotword', index, hotword);\n                \n              });\n              \n              record\n                .start({\n                  sampleRateHertz: sampleRateHertz,\n                  threshold: 0,\n                  // Other options, see https://www.npmjs.com/package/node-record-lpcm16#options\n                  verbose: false,\n                  recordProgram: 'arecord', // Try also \"arecord\" or \"sox\" or \"rec\"\n                  silence: '10.0',\n                  device: 'plughw:CARD=Device,DEV=0'\n                })\n                .on('error', console.error)\n                .pipe(detector);\n            \n\n              var recordTimeout = universe.setTimeout(()=>{\n                console.log('stopping hotword mic (timeout)...');\n                record.stop();\n                console.log('stopped hotwordmic');\n                // playSound('error');\n                doneCanWipe();\n              },10 * 1000);\n              \n              return resolve({\n                type: 'boolean:Qmdsfljk',\n                data: true\n              });\n              \n              break;\n              \n            case 'listen':\n              // starts listening and transcribing \n              // - returns/pipes result, doesn't \"hand off\" to a command, etc. \n              \n              console.log('Starting listen attempt1');\n              \n              // prevent wiping of universe (needed!)\n              universe.wipeFunc = new Promise(resolve=>{\n                doneCanWipe = resolve;\n              });\n              \n              \n              if(!googleCredsNode){\n                console.error('Missing credentials for listening/transcribing via google');\n                return reject({});\n              }\n              \n              // Creates a client\n              const client = new speech.SpeechClient({\n                // apiKey: 'AIzaSyB6KYZNp5LReVN3aSlYGoVXqcfHHdYKmLs',\n                projectId: googleCredsNode.data.project_id,\n                credentials: googleCredsNode.data\n              });\n                          \n              const encoding = 'LINEAR16';\n              const sampleRateHertz = 16000;\n              const languageCode = 'en-US';\n            \n              const request = {\n                config: {\n                  encoding: encoding,\n                  sampleRateHertz: sampleRateHertz,\n                  languageCode: languageCode,\n                },\n                interimResults: true, // If you want interim results, set this to true\n              };\n            \n              // Create a recognize stream\n              const recognizeStream = client\n                .streamingRecognize(request)\n                .on('error', console.error)\n                .on('data', data =>{\n                  // console.log(\n                  //   data.results[0] && data.results[0].alternatives[0]\n                  //     ? `Transcription: ${data.results[0].alternatives[0].transcript}\\n`\n                  //     : `\\n\\nReached transcription time limit\\n`\n                  // )\n                  if(data.results && data.results.length){\n                    let result = data.results[0];\n                    // console.log('Results:', JSON.stringify(data.results,null,2));\n                    if(result.isFinal){\n                      console.log('Final:', result.alternatives[0].transcript, result.alternatives[0].confidence)\n                      // makes sure listening stops\n                      console.log('stopping recording...');\n                      universe.clearTimeout(recordTimeout);\n                      record.stop();\n                      console.log('stopped recording');\n                      playSound('processing');\n                      doneCanWipe();\n                    } else {\n                      console.log('Temp:', result.alternatives[0].transcript, result.alternatives[0].confidence);\n                    }\n                  }\n                });\n                \n              // Play the \"we're starting to record\" sound \n              // - we want a simple way of triggering the 'play \"ready\" sound' from the usbspeaker \n              playSound('ready');\n              \n              console.log('--Starting Recording!--');\n            \n              // Start recording and send the microphone input to the Speech API\n              record\n                .start({\n                  sampleRateHertz: sampleRateHertz,\n                  threshold: 0,\n                  // Other options, see https://www.npmjs.com/package/node-record-lpcm16#options\n                  verbose: false,\n                  recordProgram: 'arecord', // Try also \"arecord\" or \"sox\" or \"rec\"\n                  silence: '10.0',\n                  device: 'plughw:CARD=Device,DEV=0'\n                })\n                .on('error', console.error)\n                .pipe(recognizeStream);\n            \n\n              var recordTimeout = universe.setTimeout(()=>{\n                console.log('stopping recording (timeout)...');\n                record.stop();\n                console.log('stopped');\n                playSound('error');\n                doneCanWipe();\n              },10 * 1000);\n              // var request = {\n              //   config: { \n              //     encoding: 'LINEAR16',\n              //     sampleRateHertz: 16000,\n              //     languageCode: 'en-US'\n              //   },\n              //   singleUtterance: false, \n              //   interimResults: true \n              // }; \n              \n              // var busy = false; \n              // function utterToMe() {\n              //   if (busy == true) { \n              //     return; \n              //   }\n               \n              //   busy = true; \n              //   const arecord = spawn('arecord', ['-f','S16_LE','-r','16000', '-D','plughw:CARD=Device,DEV=0'], {}, () => { console.log(\"Started listening...\"); });\n              \n              \n              //   // Stream the audio to the Google Cloud Speech API\n              //   const recognizeStream = client\n              //     .streamingRecognize(request)\n              //     .on('error', console.error)\n              //     .on('data', data => {\n              //       console.log('Got data');\n              //       if(data.results && \n              //         data.results.length && \n              //         data.results[0].alternatives &&\n              //         data.results[0].alternatives.length){\n              //         console.log(\n              //           `Transcription: ${data.results[0].alternatives[0].transcript}`\n              //         );\n              //       }\n              //       // console.log(\n              //       //   `Transcription: ${data.results[0].alternatives[0].transcript}`\n              //       // );\n              //     });\n              \n              //   // // Stream an audio file from disk to the Speech API, e.g. \"./resources/audio.raw\"\n              //   // fs.createReadStream(filename).pipe(recognizeStream);\n              \n              //   console.log(\"Kicked off transcribing process.\"); \n              //   arecord.stdout.pipe(recognizeStream)\n                \n              //     // .on('error', console.error) \n              //     // .on('data', function(data) { \n              //     //   if (data.results) { \n              //     //     console.log(data.results);\n              //     //   }\n              //     // });\n              \n              //   universe.setTimeout(() => { \n              //     console.log(\"Done with current recognition.\");\n              //     arecord.kill();\n              //     busy = false;\n              //   }, 5000);\n              // }\n              // utterToMe();\n              \n              return resolve({\n                type: 'boolean:Qmdsfljk',\n                data: true\n              });\n            \n            default:\n              console.log('Unhandled inputOpts.action type:', inputOpts.action);\n              break;\n              \n          }\n          return resolve({\n            type: 'boolean:Qmdsfdlkj',\n            data: true\n          });\n          break;\n          \n        default:\n          console.error('Invalid action type:', inputAction);\n          return reject({});\n      }\n        \n    }catch(err){\n      console.error('err managing device:', err);\n      resolve({ERROR: true, err: err.toString()});\n    }\n    \n    \n  })\n})()"
  }
}